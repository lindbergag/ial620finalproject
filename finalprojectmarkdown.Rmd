---
title: "Sentiment Analysis IAL 620"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Check whether necessary libraries are installed. If not, install them. Otherwise, load the libraries
if (!require("tidytext")) install.packages("tidytext")
library(tidytext)
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if (!require("tm")) install.packages("tm")
library(tm)
if (!require("lubridate")) install.packages("lubridate")
library(lubridate)
if (!require("wordcloud")) install.packages("wordcloud")
library(wordcloud)
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)
if (!require("scales")) install.packages("scales")
library(scales)
if (!require("ggthemes")) install.packages("ggthemes")
library(ggthemes)
if (!require("zoo")) install.packages("zoo")
library(zoo)
if (!require("forcats")) install.packages("forcats")
library(forcats)
if (!require("reshape2")) install.packages("reshape2")
library(reshape2)
if (!require("igraph")) install.packages("igraph")
library(igraph)
if (!require("ggraph")) install.packages("ggraph")
library(igraph)
if (!require("widyr")) install.packages("widyr")
library(widyr)
if (!require("topicmodels")) install.packages("topicmodels")
library(topicmodels)
if (!require("ldatuning")) install.packages("ldatuning")
library(ldatuning)

theme_set(theme_wsj())
```
Import the Dataset:
Describe Steps already taken
```{r}
# Open the rds file and convert it to a tibble
raw.data <- readRDS(file='breneebrown.rds')

#Convert to a tibble
scrape.data <- as_tibble(raw.data)

#Drop the last row because it's a trailer
scrape.data <- scrape.data[1:31,]

```

Clean host, guest, and title meta data
```{r}
#Separate speakers from title
scrape.data <- scrape.data %>%
  separate(title, c("speakers", "title"), sep = "( on)|:") %>%
  mutate(title = coalesce(title, speakers))

#Clean Host and Guest Speaker names
scrape.data <- scrape.data %>%
  separate(speakers, c("host", "guest"), sep = " with ")

scrape.data$host <- "Brené Brown"

scrape.data$guest <- removeWords(scrape.data$guest, c("Episode Transcript", "Trailer"))

#Some episodes are monologues from Brene, set the guest as Brene
scrape.data$guest[is.na(scrape.data$guest)] <- "Brené Brown"
```

Clean title
```{r}
#clean title names
scrape.data$title <- stripWhitespace(scrape.data$title)
scrape.data$title <- str_trim(scrape.data$title)

capwords <- function(s, strict = FALSE) {
    cap <- function(s) paste(toupper(substring(s, 1, 1)),
                  {s <- substring(s, 2); if(strict) tolower(s) else s},
                             sep = "", collapse = " " )
    sapply(strsplit(s, split = " "), cap, USE.NAMES = !is.null(names(s)))
}

scrape.data$title <- scrape.data$title %>%
  capwords
```


Clean Date
```{r}
scrape.data$date <- removeWords(scrape.data$date, "Episode Transcript")
scrape.data$date <- str_sub(scrape.data$date, 3)
scrape.data$date <- stripWhitespace(scrape.data$date)

# uses lubridate package to convert UTC datetime format
scrape.data$date <- mdy(scrape.data$date, tz = NULL)

# Add month and week
scrape.data <- scrape.data %>%
  group_by(month=floor_date(date, "month"), week = week(date), year = year(date))

# transform month to month name abbreviation
scrape.data$month <- scrape.data$month %>%
  month(label = TRUE)

timeline <- scrape.data$date %>%
  as.yearmon() %>%
  unique() %>%
  as.factor()

```


Clean text data
```{r}
#create a list of text box names
text_columns <- seq(1, 1000)
text_columns <- text_columns %>%
  as.character()

#Separate the text by speaker
scrape.data <- scrape.data %>%
  separate(text, into = text_columns, sep = "\n")

#Gather the text into tidy format
scrape.data <- scrape.data %>%
  gather(key = "speaker_num", value = "text", 8:956)


#drop the columns that aren't useful
scrape.data <- scrape.data[ -c(6:56) ]


#Convert to a data frame
scrape.data <- data.frame(scrape.data)


#strip whitespace
scrape.data$text <- stripWhitespace(scrape.data$text)
#replace blank spaces with NA
scrape.data$text[scrape.data$text == " "] <- NA
scrape.data$text[scrape.data$text == ""] <- NA
scrape.data$text[scrape.data$text == "[:blank:]"] <- NA
#replace [music] with NA
scrape.data$text[scrape.data$text == "[music]"] <- NA
scrape.data$text[scrape.data$text == "[Music]"] <- NA
#replace transcript with NA
scrape.data$text[scrape.data$text == "Transcript"] <- NA
#Remove NA words
scrape.data <- scrape.data %>%
  drop_na(text)


#convert speaker number to numeric
scrape.data$speaker_num <- as.integer(scrape.data$speaker_num)

#Remove Credits
scrape.data <- scrape.data %>%
  group_by(title) %>%
  mutate(last.speaker = max(speaker_num))

scrape.data <- scrape.data %>%
  group_by(title) %>%
  filter(speaker_num != last.speaker)

spotify.original <- "Spotify original from Parcast"

scrape.data <- scrape.data %>%
  group_by(title) %>%
  filter(!(str_detect(text, spotify.original)))

scrape.data <- scrape.data %>%
  filter(!(str_detect(text, "©")))

#Drop the last speaker variable
scrape.data <- scrape.data[-11]
```
clean speaker data
```{r}
#Extract who the speaker is, Brene or her guest
scrape.data <- scrape.data%>%
  extract(text, "speaker", "(BB)", remove = FALSE)

#The first time a speaker's name is used, their full name is used. Slice their full name as the speaker
scrape.data$speaker <- sapply(strsplit(as.character(scrape.data$text), ":"),"[", 1)

#For the episodes that are monologues, set the speaker as Brene
scrape.data$speaker[scrape.data$guest == scrape.data$host] <- "BB"

#Couple of noteworthy spots
scrape.data$speaker[str_detect(scrape.data$speaker, "Before we jump")] <- "BB"
scrape.data$speaker[str_detect(scrape.data$speaker, "Okay, ")] <- "BB"

scrape.data$speaker[str_length(scrape.data$speaker) > 250] <- "AC"

#drop these rows, they don't add any semantic value
scrape.data <- scrape.data %>%
  filter(str_length(speaker) < 25)

#Convert all Brene Browns to BB
scrape.data$speaker[scrape.data$speaker == "Brené Brown"] <- "BB"

#Convert all guests to their initials
scrape.data <- scrape.data %>%
  mutate(speaker = gsub('[a-z]', "", speaker)) %>%
  mutate(speaker = gsub('[[:space:]]', "", speaker)) %>%
  separate(speaker, c(NA, "speaker"), "[[:punct:]]", fill = "left")

#Replace Blanks with white NA
scrape.data$speaker[scrape.data$speaker == " "] <- NA
scrape.data$speaker[scrape.data$speaker == ""] <- NA
scrape.data$speaker[scrape.data$speaker == "[:blank:]"] <- NA
#Remove NA speakers
scrape.data <- scrape.data %>%
  drop_na(speaker)

#create a new column, host or guest that tags whether the speaker is the host or a guest.
scrape.data$host_or_guest[scrape.data$speaker == "BB"] <- "host"

#If the speaker isn't the host, it's the guest
scrape.data$host_or_guest[is.na(scrape.data$host_or_guest)] <- "guest"

```

Cleaning the text more
```{r}
#Drop the speaker names from the text
scrape.data <- scrape.data %>%
  separate(text, c(NA, "text"), sep = ": ", fill = "left")

#clean up the speaker number per episode
scrape.data <- scrape.data %>%
  group_by(title) %>%
  mutate(speaker_num = row_number())

#create a total lines, ever
scrape.data <- scrape.data %>%
  ungroup()%>%
  arrange(date) %>%
  mutate(total_num = row_number())

#back-up so I don't have to do these steps over and over
clean.data.bkup <- scrape.data
```

```

```
Most Common Words
```{r}
# Tokenize the text with one word per token so we're in tidytext format
tokenized.data <- clean.data.bkup %>%
  unnest_tokens(word, text)

# Glance at the most common words
tokenized.data %>%
  count(word, sort = TRUE)
```

There are a lot of stop words, need to clean up this data
```{r}

clean.data <- clean.data.bkup

# removes carrige returns and new lines from text
clean.data$text <- gsub("\r?\n|\r", " ", clean.data$text)

# removes punctuation
clean.data$text <- gsub("[[:punct:]]", "", clean.data$text)

# forces entire corpus to lowercase
clean.data$text <- tolower(clean.data$text)

#removes numbers from text
clean.data$text <- removeNumbers(clean.data$text)

# remove stop words
clean.data$text <- removeWords(clean.data$text, stopwords("SMART"))

# removes additional remaining whitespace
clean.data$text <- stripWhitespace(clean.data$text)

# Tokenize the text with one word per token so we're in tidytext format
tokenized.data <- clean.data %>%
  unnest_tokens(word, text)

# Glance at the most common words
tokenized.data %>%
  count(word, sort = TRUE)
```



Better, but still problematic
```{r}
#Remove speaker names
speaker.names <- c((unique(clean.data.bkup$guest)))
# removes carrige returns and new lines from speaker names
speaker.names <- gsub("\r?\n|\r|\r?\t", " ", speaker.names)
# removes punctuation
speaker.names <- gsub("[[:punct:]]", "", speaker.names)
# forces entire list to lowercase
speaker.names <- tolower(speaker.names)
#remove white space
speaker.names <- stripWhitespace(speaker.names)
speaker.names <- str_trim(speaker.names)
#flatten
speaker.names <- str_flatten(speaker.names, collapse = " ")
#spearate
speaker.names <- unlist(strsplit(speaker.names, " "))

# remove additional words
other.words <- c(speaker.names , "yeah", "don", "ve", "things", "thing", "lot", "ll", "didn", "brené", "brown", "podcast",
                 "episode", "page")

clean.data$text <- removeWords(clean.data$text, other.words)


# Tokenize the text with one word per token so we're in tidytext format
tokenized.data <- clean.data %>%
  unnest_tokens(word, text)

# Glance at the most common words
tokenized.data %>%
  count(word, sort = TRUE)
```


```{r}

#Visual Most common Words
tokenized.data %>%
  count(word, sort = TRUE) %>%
  filter(n > 250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Occurences of Words")
```
```{r}
tokenized.data %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
Most Common Bigrams
```{r}
bigram.data <- clean.data.bkup

# Tokenize the text with two word per token so we're in tidytext format
bigram.data <- bigram.data %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Glance at the most common word pairs
bigram.data %>%
  count(bigram, sort = TRUE)

```
Mostly stop words, no surprise

```{r}

#Load stopwords
data(stop_words)

#Create custom stopwords list

lexicon <-  rep("custom", times=length(other.words))
my.stop.words <- data.frame(other.words, lexicon)
names(my.stop.words) <- c("word", "lexicon")

# Add the dataframe to stop_words df that exists in the library stopwords
stop_words <-  bind_rows(stop_words, my.stop.words)

##Reset and clean the data
bigram.data <- clean.data.bkup

# removes carrige returns and new lines from text
bigram.data$text <- gsub("\r?\n|\r", " ", bigram.data$text)

# removes punctuation
bigram.data$text <- gsub("[[:punct:]]", "", bigram.data$text)

# forces entire corpus to lowercase
bigram.data$text <- tolower(bigram.data$text)

# Change the apostrophes because ’ and ' are different characters, and we're going to compare using '
bigram.data$text <- gsub("’", "'", bigram.data$text, ignore.case=TRUE)

# Tokenize the text with two word per token so we're in tidytext format
bigram.data <- bigram.data %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigrams.separated <- bigram.data %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams.filtered <- bigrams.separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  drop_na()

# new bigram counts:
bigram.counts <- bigrams.filtered %>% 
  count(word1, word2, sort = TRUE)

bigram.counts

```


```{r}
#Visual Most common Bigrams
bigrams.filtered %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)%>%
  filter(n > 15) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Occurences of Bigrams")
```

This is fun, but I'm seeing 1 big, interesting challenge. As a host, Brene often goes into "rapid-fire" questions. What would this data look like as just the host, or just the guest? We'll come back to that in the comparative section of our research

Let's look at sentiment analysis
```{r}
#Bing Lexicon is simply positive or negative, good or bad. Using it for initial data exploration

# Get negative sentiments for bing lexicon
bing.negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

# count the top bing lexicon negatives
negative <- tokenized.data %>%
  inner_join(bing.negative) %>%
  count(word, sort = TRUE)


#The following words most contribute to negative sentiments
negative
```

```{r}
# Assign and calculate sentiment data
sentiment.all <- tokenized.data %>%
  inner_join(get_sentiments("bing")) %>%
  count(month, index = total_num, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

# Sentiment of the podcast from day 1 to ~current
ggplot(sentiment.all, aes(index, sentiment, fill = as.factor(month))) +
  theme(plot.title.position = "plot",
        plot.title = element_text(size = 28),
        legend.position = "none") +
  geom_col(width = 1) +
  scale_fill_viridis_d() +
  scale_x_continuous(breaks = c(1, 1500, 3000, 4500, 6000),  labels = c(timeline[1], timeline[3], timeline[5], timeline[7], timeline[9])) +
  ggtitle("Sentiment of Podcast Over Time")
```



```{r}
# find out how much each word contributed to sentiment
bing.word.counts <- tokenized.data %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# visualize top words contributing to positive or negative sentiment
bing.word.counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = NULL) +
  theme(strip.text = element_text(family = "mono", size = 12, face = "bold")) +
  ggtitle("Most Impactful Negative Words") +
  coord_flip()

```
```{r}

#wordcloud because pretty
tokenized.data %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

How much are negation words affecting things?
```{r}
negation.words <- c("not", "no", "never", "without")

negated.words <- bigrams.separated %>%
  filter(word1 %in% negation.words) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

negated.words %>%
  mutate(contribution = n * value) %>%
  group_by(word1) %>%
  arrange(desc(abs(contribution))) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"") +
  facet_wrap(~word1, scales = "free_y") +
  ggtitle("Most Common Negated Words") +
  theme(strip.text = element_text(family = "mono", size = 12, face = "bold"),
        plot.title.position = "plot")
```

what about bigrams?

```{r}
#make it tidy

#label all of the negative word 1s
bigram.word1 <- bigrams.filtered %>%
  inner_join(get_sentiments("bing"), by=c(word1 = "word")) %>%
  rename(sentiment1 = sentiment)

#label all of the negative word 2s
bigram.word2 <- bigrams.filtered %>%
  inner_join(get_sentiments("bing"), by = c(word2 = "word")) %>%
  rename(sentiment2 = sentiment)

#If sentiment 1 is positive and sentiment 2 is negative, NA
#If 1 or both is positive and the other is NA, positive
#If 1 or both is negative and the other is NA, negative
bigram.sentiment <- merge(bigram.word1, bigram.word2, by = c("word1", "word2"), all = TRUE) %>%
  mutate(sentiment = ifelse(sentiment1 == "positive" & sentiment2 == "positive" |
                            sentiment1 == "positive" & is.na(sentiment2) |
                              is.na(sentiment1) & sentiment2 == "positive", "positive",
                            ifelse(sentiment1 == "negative" & sentiment2 == "negative"|
                                     sentiment1 == "negative" & is.na(sentiment2) |
                                     is.na(sentiment1) & sentiment2 == "negative", "negative", NA))) %>%
  drop_na(sentiment)

#rejoin, and count
bigram.sentiment.time <- bigram.sentiment %>%
  mutate(total_num = ifelse(is.na(total_num.x), total_num.y, total_num.x)) %>%
  mutate(month = ifelse(is.na(month.x), month.y, month.x)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(month, index = total_num, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)



# Sentiment of bigrams for the podcast from day 1 to ~current
bigram.sentiment.time %>%
  filter(sentiment < 10 & sentiment > -10) %>%
ggplot(aes(index, sentiment, fill = as.factor(month))) +
  theme(plot.title.position = "plot",
        plot.title = element_text(size = 28),
        legend.position = "none") +
  geom_col(width = 1) +
  scale_fill_viridis_d() + 
  scale_x_continuous(breaks = c(1, 1500, 3000, 4500, 6000),  labels = c(timeline[1], timeline[3], timeline[5], timeline[7], timeline[9])) +
  ggtitle("Sentiment of Bigrams Over Time")

```

```{r}

#rejoin, and count
bigram.sentiment.count <- bigram.sentiment %>%
  mutate(total_num = ifelse(is.na(total_num.x), total_num.y, total_num.x)) %>%
  mutate(month = ifelse(is.na(month.x), month.y, month.x)) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sentiment, sort = TRUE) %>%
  ungroup()


# find out how much each word contributed to sentiment
bing.word.counts <- tokenized.data %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()


# visualize top bigrams contributing to positive or negative sentiment
bigram.sentiment.count %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = NULL) +
    theme(strip.text = element_text(family = "mono", size = 12, face = "bold"),
          plot.title.position = "plot") +
  ggtitle("Bigrams Contributing to Sentiment") +
  coord_flip()

```


Let's take a look at per episode tf-idf
```{r}

#generate word counts per title
title.word.counts <- tokenized.data %>%
  count(title, word, sort = TRUE)

#use bind_tf_idf to get tf, idf, and tf_idf
title.tf.idf <- title.word.counts %>%
  bind_tf_idf(word, title, n)

#generate a plot. This won't look pretty in markdown.
title.tf.idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~title, ncol = 6, scales = "free", labeller = label_wrap_gen(20)) +
    theme(strip.text = element_text(family = "mono", size = 12, face = "bold"),
        strip.background = element_rect(fill = "light gray", linetype = "dotted"),
        plot.title.position = "plot",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  scale_fill_viridis_d()+
  labs(x = "tf-idf", y = NULL)
```
What are the most unique words across the corpus?
```{r}

title.tf.idf %>%
  slice_max(tf_idf, n = 20) %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf))) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  ggtitle("Words with Highest tf-idf")

```

Visualizing Bigrams
```{r}
bigram.graph <- bigram.counts %>%
  filter(n > 12) %>%
  graph_from_data_frame()

ggraph(bigram.graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

What about directionality?
```{r}
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram.graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

What about correlation?
```{r}
word.pairs <- tokenized.data %>%
  pairwise_count(word, title, sort = TRUE)

word.cors <- tokenized.data %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, title, sort = TRUE)

word.cors
```

What words are most associated with
```{r}

word.cors %>%
  filter(item1 %in% c("story", "listening", "conversation", "vulnerability")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
    theme(strip.text = element_text(family = "mono", size = 12, face = "bold"),
        plot.title.position = "plot") +
  ggtitle("Word Correlations") +
  coord_flip()

```

Topic Modeling
```{r}

#Convert data into dtm
bb.dtm <- tokenized.data %>%
  mutate(document = title) %>%
  count(document, word, sort = TRUE) %>%
  ungroup() %>%
  cast_dtm(document, word, n)

#create an LDA model
bb.lda <- LDA(bb.dtm, k = 31, control = list(seed = 1234))

#Per Topic Word Probablilities
title.topics <- tidy(bb.lda, matrix = "beta")

#Isolate top 5 terms within each topic
bb.top.terms <- title.topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)
```


graph
```{r}
bb.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
    theme(strip.text = element_text(family = "mono", size = 12, face = "bold")) +
  scale_fill_viridis_d()

```

per document classification
```{r}
title.gamma <- tidy(bb.lda, matrix = "gamma")
title.gamma
```

Box plots visualising the topic per episode
```{r}
title.gamma %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title, ncol = 3) +
  labs(x = "topic", y = expression(gamma)) +
    theme(strip.text = element_text(family = "mono", size = 12, face = "bold"),
        strip.background = element_rect(fill = "light gray", linetype = "dotted"),
        plot.title.position = "plot",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())
```


Classify each episode
```{r}
title.classifications <- title.gamma %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

```


Generate consensus
```{r}
title.topics <- title.classifications %>%
  count(document, topic) %>%
  group_by(document) %>%
  slice_max(n, n = 1) %>% 
  ungroup() %>%
  transmute(consensus = document, topic)

title.topics

```

Were any titles mislabeled?
```{r}
title.gamma %>%
  inner_join(title.topics, by = "topic") %>%
  filter(document != consensus)
```

Find which words were assigned to which word pairs
```{r}
assignments <- augment(bb.lda, data = bb.dtm)
assignments
```

Join the word assignments with the consensus topics to see which words were incorrectly classified
```{r}
assignments <- assignments %>%
  inner_join(title.topics, by = c(".topic" = "topic"))

assignments
```

correlation matrix for topic model
```{r}
assignments %>%
  count(document, consensus, wt = count) %>%
  mutate(across(c(document, consensus), ~str_wrap(., 20))) %>%
  group_by(document) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, document, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "darkred", label = percent_format()) +
  scale_y_discrete(label = function(x) {return(gsub('[^A-Z]', "", x))}) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.position = c(.95, .1),
        legend.background = element_rect(fill = "transparent")) +
  labs(fill = "% of assignments")
```

Let's look at empirical topic selection
```{r}
# calculate metrics according to models
result <- FindTopicsNumber(
  bb.dtm,
  topics = seq(from = 2, to = 40, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 1337),
  mc.cores = 14L,
  verbose = TRUE
)

# visualize results
FindTopicsNumber_plot(result)
```

Looks like somewhere between 7 and 15 topics is a sweet spot. I'm going to roll with 11 and see how it shapes the results

```{r}
#create an LDA model
bb.small.lda <- LDA(bb.dtm, k = 11, control = list(seed = 1234))

#Per Topic Word Probablilities
emp.topics <- tidy(bb.small.lda, matrix = "beta")

#Isolate top 5 terms within each topic
emp.top.terms <- emp.topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

emp.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  theme(strip.text = element_text(family = "mono", size = 12, face = "bold")) +
  scale_fill_viridis_d()+
  scale_y_reordered()
```


```{r}
emp.gamma <- tidy(bb.small.lda, matrix = "gamma")
emp.gamma
```

empiricized gamma graph
```{r}
emp.gamma %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title, ncol = 3) +
  theme(strip.text = element_text(family = "mono", size = 12, face = "bold"),
        strip.background = element_rect(fill = "light gray", linetype = "dotted"),
        plot.title.position = "plot",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  labs(x = "topic", y = expression(gamma))
```

empiricized classification
```{r}
emp.classifications <- emp.gamma %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

```


empiricized topic model
```{r}
emp.topics <- emp.classifications %>%
  count(document, topic) %>%
  group_by(document) %>%
  slice_max(n, n = 1) %>% 
  ungroup() %>%
  transmute(consensus = document, topic)

emp.topics

```

empiricized gamma
```{r}
emp.gamma %>%
  inner_join(emp.topics, by = "topic") %>%
  filter(document != consensus)
```

use augment function to figure out document / topic assignment
```{r}
assignments.emp <- augment(bb.small.lda, data = bb.dtm)
assignments.emp
```
identify the words assigned by episode
```{r}
assignments.emp <- assignments.emp %>%
  inner_join(emp.topics, by = c(".topic" = "topic"))

assignments.emp
```
generate a correlation matrix for words assigned to the episode
```{r}
assignments.emp %>%
  count(document, consensus, wt = count) %>%
  mutate(across(c(document, consensus), ~str_wrap(., 20))) %>%
  group_by(document) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, document, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "darkred", label = percent_format()) +
  scale_y_discrete(label = function(x) {return(gsub('[^A-Z]', "", x))}) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.background = element_rect(fill = "transparent")) +
  labs(fill = "% of assignments")
```

Let's make that a table so we can process the most mis-assigned
```{r}

assignments.emp %>%
  count(document, consensus, wt = count) %>%
  mutate(across(c(document, consensus), ~str_wrap(., 20))) %>%
  mutate(percent = n / sum(n)) %>%
  filter(document != consensus) %>%
  slice_max(percent, n = 10)
```

Let's dig in on Habits and Productivity
```{r}

assignments.emp %>%
  count(document, consensus, wt = count) %>%
  mutate(across(c(document, consensus), ~str_wrap(., 20))) %>%
  mutate(percent = n / sum(n)) %>%
  filter(document == "Habits And\nProductivity" |
           consensus == "Habits And\nProductivity")
```

Let's compare these documents
clean the data first
```{r}
#Clean title names to make life easier
clean.titles <- tokenized.data

# removes carrige returns and new lines from text
clean.titles$title <- gsub("\r?\n|\r", " ", clean.titles$title)
# removes punctuation
clean.titles$title <- gsub("[[:punct:]]", "", clean.titles$title)
# forces entire corpus to lowercase
clean.titles$title <- tolower(clean.titles$title)
# removes additional remaining whitespace
clean.titles$title <- stripWhitespace(clean.titles$title)
```

visualize most common words across correlated episodes
```{r}

clean.titles %>%
  filter(title == "habits and productivity" |
           title == "creativity surrender and aesthetic force" |
           title == "the power of knowing what you dont know") %>%
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>%
  top_n(10) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Most Common Words")

```


Most common bigrams across correlated episodes
```{r}

#Clean title names to make life easier
clean.titles.bigrams <- bigrams.filtered

# removes carrige returns and new lines from text
clean.titles.bigrams$title <- gsub("\r?\n|\r", " ", clean.titles.bigrams$title)
# removes punctuation
clean.titles.bigrams$title <- gsub("[[:punct:]]", "", clean.titles.bigrams$title)
# forces entire corpus to lowercase
clean.titles.bigrams$title <- tolower(clean.titles.bigrams$title)
# removes additional remaining whitespace
clean.titles.bigrams$title <- stripWhitespace(clean.titles.bigrams$title)
```

Visualize most common bigrams across correlated episodes

```{r}

#Visual Most common Bigrams
clean.titles.bigrams %>%
  filter(title == "habits and productivity" |
           title == "creativity surrender and aesthetic force" |
           title == "the power of knowing what you dont know") %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)%>%
  top_n(10) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL) +
  theme(plot.title.position = "plot") +
  ggtitle("Occurences of Bigrams")

```

These words are common amongst them, but where are they different?
Per Episode most common words
```{r}
clean.titles %>%
  filter(title == "habits and productivity" |
           title == "creativity surrender and aesthetic force" |
           title == "the power of knowing what you don’t know") %>%
  count(word, title, sort = TRUE) %>%
  group_by(title) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = title)) +
  geom_col(show.legend = FALSE) +
  scale_fill_wsj('colors6') +
  labs(y = NULL) +
  facet_wrap(~title, scales = "free_y", labeller = label_wrap_gen(20)) +
  theme(strip.text = element_text(family = "mono", size = 12, face = "bold"),
        plot.title.position = "plot")+
  ggtitle("Most Common Words")
```

generate bigram word counts for these episodes
```{r}
bigram.clean.counts <- clean.titles.bigrams %>%
  filter(title == "habits and productivity" |
           title == "creativity surrender and aesthetic force" |
           title == "the power of knowing what you don’t know") %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, title, sort = TRUE)
```
Most common bigrams correlated episodes
```{r}
bigram.clean.counts %>%
  group_by(title) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n, fill = title)) +
  geom_col(show.legend = FALSE) +
  scale_fill_wsj('colors6') +
  scale_x_discrete(labels = wrap_format(1)) +
  facet_wrap(~title, scales = "free", labeller = label_wrap_gen(20)) +
  labs(x = NULL) +
  theme(strip.text = element_text(family = "mono", size = 12, face = "bold"))+
  ggtitle("Most Common Bigrams") +
  coord_flip()
```

```{r}
#generate word counts per title
clean.title.word.counts <- clean.titles %>%
  filter(title == "habits and productivity" |
           title == "creativity surrender and aesthetic force" |
           title == "the power of knowing what you don’t know") %>%
  count(word, title, sort = TRUE)

#use bind_tf_idf to get tf, idf, and tf_idf
clean.title.tf.idf <- clean.title.word.counts %>%
  bind_tf_idf(word, title, n)
```

plot the words with the highest tf-idf for these episodes
```{r}
clean.title.tf.idf %>%
  slice_max(tf_idf, n = 20) %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf))) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  ggtitle("Words with Highest tf-idf") +
  theme(plot.title.position = "plot")
```

Most Impactful words per document in these titles
```{r}
#generate a plot.
clean.title.tf.idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
  geom_col(show.legend = FALSE) +
  scale_fill_wsj('colors6') +
  facet_wrap(~title, scales = "free", labeller = label_wrap_gen(20)) +
  labs(x = "tf-idf", y = NULL) +
  theme(strip.text = element_text(family = "mono", size = 12, face = "bold")) +
  ggtitle("High tf-idf")
```

generate word frequencies across these episodes
```{r}
freq <- clean.titles %>%
  filter(title == "habits and productivity" |
           title == "creativity surrender and aesthetic force" |
           title == "the power of knowing what you don’t know") %>%
  count(word, title, sort = TRUE) %>%
  group_by(title) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = title, values_from = proportion) %>%
  pivot_longer(`habits and productivity`:`creativity surrender and aesthetic force`,
               names_to = "title", values_to = "proportion")

```

visualize word frequencies across these episodes
```{r}
ggplot(freq, aes(x = proportion, y = `the power of knowing what you don’t know`, 
                      color = abs(`the power of knowing what you don’t know` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001),
                       low = "black", high = "#018954") +
  facet_wrap(~title, ncol = 2) +
  theme(legend.position="none",
        axis.title.y = element_text(size = 12, face = "bold"),
        strip.text = element_text(family = "mono", face = "bold", size = 12),
        plot.title.position = "plot") +
  labs(y = "the power of knowing what you don’t know", x = NULL) +
  ggtitle("Comparing Episodic Word Frequencies")

```


Brene Asks some specific questions that I think are interesting:
Starting with Best or Worst Leadership Advice
```{r}
#Start Fresh
questions.answers <- clean.data.bkup

# forces entire corpus to lowercase
questions.answers$text <- tolower(questions.answers$text)

#The answers to these questions come after Brene asks about "one piece of leadership advice"
leader.answers <- questions.answers$total_num[str_detect(questions.answers$text, "leadership advice") & questions.answers$speaker == "BB"]+1

#Filter for just the answers
leader.a <- questions.answers %>%
  filter(total_num %in% leader.answers)
```

Clean answer data
```{r}

# removes carrige returns and new lines from text
leader.a$text <- gsub("\r?\n|\r", " ", leader.a$text)
# removes punctuation
leader.a$text <- gsub("[[:punct:]]", "", leader.a$text)
#removes numbers from text
leader.a$text <- removeNumbers(leader.a$text)
# remove stop words
leader.a$text <- removeWords(leader.a$text, stopwords("SMART"))
# removes additional remaining whitespace
leader.a$text <- stripWhitespace(leader.a$text)
#Remove additional words
leader.a$text <- removeWords(leader.a$text, other.words)

#Filter repeating the question
question <- c("piece", "leadership", "advice")
leader.a$text <- removeWords(leader.a$text, question)

# Tokenize the text with one word per token so we're in tidytext format
tokenized.leader <- leader.a %>%
  unnest_tokens(word, text)

# Glance at the most common words
tokenized.leader %>%
  count(word, sort = TRUE)
```

most common words in leadership advice
```{r}
#Visual Most common Words
tokenized.leader %>%
  count(word, sort = TRUE) %>%
  filter(n > 3) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Best or Worst Leadership Advice")
```

```{r}
tokenized.leader %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

generate word correlations for answers to best or worst leadership advice
```{r}
leader.cors <- tokenized.leader %>%
  group_by(word) %>%
  filter(n() >= 3) %>%
  pairwise_cor(word, title, sort = TRUE)

leader.cors
```
correlation chart for leadership advice
```{r}
leader.cors %>%
  filter(item1 %in% c("time", "energy", "listening", "spend")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  theme(legend.position="none",
        strip.text = element_text(family = "mono", size = 14, face = "bold"),
        plot.title.position = "plot")+
  ggtitle("Word Correlations for Leadership Advice") +
  coord_flip()
```

Look at Bigrams
```{r}

##Reset and clean the data
#Filter for just the answers
leader.bigrams <- questions.answers %>%
  filter(total_num %in% leader.answers)

# removes carrige returns and new lines from text
leader.bigrams$text <- gsub("\r?\n|\r", " ", leader.bigrams$text)
# removes punctuation
leader.bigrams$text <- gsub("[[:punct:]]", "", leader.bigrams$text)
# Change the apostrophes because ’ and ' are different characters, and we're going to compare using '
leader.bigrams$text <- gsub("’", "'", leader.bigrams$text, ignore.case=TRUE)

#Create custom stopwords list
lexicon.leader <-  rep("custom", times=length(question))
leader.stop.words <- data.frame(question, lexicon.leader)
names(leader.stop.words) <- c("word", "lexicon")

# Add the dataframe to our existing stop_words dataframe in a new dataframe
leader.stop_words <-  bind_rows(stop_words, leader.stop.words)


# Tokenize the text with two word per token so we're in tidytext format
leader.bigrams <- leader.bigrams %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

#Separate the words
leader.bigram.separated <- leader.bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

#Filter stop words
leader.bigrams.filtered <- leader.bigram.separated %>%
  filter(!word1 %in% leader.stop_words$word) %>%
  filter(!word2 %in% leader.stop_words$word) %>%
  drop_na()

# new bigram counts:
leader.bigram.counts <- leader.bigrams.filtered %>% 
  count(word1, word2, sort = TRUE)

leader.bigram.counts
```

Visualize the relationship between words
```{r}
leader.bigram.graph <- leader.bigram.counts %>%
  graph_from_data_frame()

ggraph(leader.bigram.graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```
Add directionality
```{r}
ggraph(leader.bigram.graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

Repeat for Vulnerability
```{r}
#The answers to these questions come after Brene asks about "vulnerability is"
vulnerability.answers <- questions.answers$total_num[str_detect(questions.answers$text, "vulnerability is") & questions.answers$speaker == "BB"]+1

#Filter for just the answers
vulnerability.a <- questions.answers %>%
  filter(total_num %in% vulnerability.answers)
```
Clean answer data
```{r}
# removes carrige returns and new lines from text
vulnerability.a$text <- gsub("\r?\n|\r", " ", vulnerability.a$text)
# removes punctuation
vulnerability.a$text <- gsub("[[:punct:]]", "", vulnerability.a$text)
#removes numbers from text
vulnerability.a$text <- removeNumbers(vulnerability.a$text)
# remove stop words
vulnerability.a$text <- removeWords(vulnerability.a$text, stopwords("SMART"))
# removes additional remaining whitespace
vulnerability.a$text <- stripWhitespace(vulnerability.a$text)
#Remove additional words
vulnerability.a$text <- removeWords(vulnerability.a$text, other.words)

#Filter repeating the question
vuln.question <- c("vulnerability")
vulnerability.a$text <- removeWords(vulnerability.a$text, vuln.question)

# Tokenize the text with one word per token so we're in tidytext format
tokenized.vulnerability <- vulnerability.a %>%
  unnest_tokens(word, text)

# Glance at the most common words
tokenized.vulnerability %>%
  count(word, sort = TRUE)
```

Bar charts for most common words in vulnerability answers
```{r}
#Visual Most common Words
tokenized.vulnerability %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Vulnerability is...")
```

word cloud for vulnerability answers
```{r}
tokenized.vulnerability %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

Generate Correlation data for vulnerability answers
```{r}
vulnerability.cors <- tokenized.vulnerability %>%
  group_by(word) %>%
  filter(n() >= 3) %>%
  pairwise_cor(word, title, sort = TRUE)

vulnerability.cors
```

Correlation charts for Vulnerability Answers
```{r}
vulnerability.cors %>%
  filter(item1 %in% c("courage", "feel", "heart", "work")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  theme(plot.title.position = "plot",
        strip.text = element_text(family = "mono", size = 16, face = "bold")) +
  ggtitle("Vulnerability Correlations") +
  coord_flip()
```


Bigrams related to vulnerability answers
```{r}

##Reset and clean the data
#Filter for just the answers
vulnerability.bigrams <- questions.answers %>%
  filter(total_num %in% vulnerability.answers)

# removes carrige returns and new lines from text
vulnerability.bigrams$text <- gsub("\r?\n|\r", " ", vulnerability.bigrams$text)
# removes punctuation
vulnerability.bigrams$text <- gsub("[[:punct:]]", "", vulnerability.bigrams$text)
# Change the apostrophes because ’ and ' are different characters, and we're going to compare using '
vulnerability.bigrams$text <- gsub("’", "'", vulnerability.bigrams$text, ignore.case=TRUE)

#Create custom stopwords list
lexicon.vulnerability <-  rep("custom", times=length(vuln.question))
vulnerability.stop.words <- data.frame(question, lexicon.vulnerability)
names(vulnerability.stop.words) <- c("word", "lexicon")

# Add the dataframe to our existing stop_words dataframe in a new dataframe
vulnerability.stop_words <-  bind_rows(stop_words, vulnerability.stop.words)


# Tokenize the text with two word per token so we're in tidytext format
vulnerability.bigrams <- vulnerability.bigrams %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

#Separate the words
vulnerability.bigram.separated <- vulnerability.bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

#Filter stop words
vulnerability.bigrams.filtered <- vulnerability.bigram.separated %>%
  filter(!word1 %in% vulnerability.stop_words$word) %>%
  filter(!word2 %in% vulnerability.stop_words$word) %>%
  drop_na()

# new bigram counts:
vulnerability.bigram.counts <- vulnerability.bigrams.filtered %>% 
  count(word1, word2, sort = TRUE)

vulnerability.bigram.counts
```

Bigram Graph for Vulnerability Answers
```{r}
vulnerability.bigram.graph <- vulnerability.bigram.counts %>%
  graph_from_data_frame()

ggraph(vulnerability.bigram.graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```
Directional Bigram Graph for Vulnerability Answers
```{r}
ggraph(vulnerability.bigram.graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

Compare Word Frequencies Between Answers to Vulnerability and Leadership
```{r}

frequency <- bind_rows(mutate(tokenized.leader, subject = "leadership"),
                       mutate(tokenized.vulnerability, subject = "vulnerability")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(subject, word) %>%
  group_by(subject) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = subject, values_from = proportion)

frequency


```
Visualize the frequency of words in the answers to Brene's questions regarding leadership and vulnerability
```{r}

ggplot(frequency, aes(x = leadership, y = vulnerability, 
                      color = abs(leadership - vulnerability))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.005),
                       low = "black", high = "#018954") +
  theme(legend.position="none",
        axis.title = element_text(size = 16)) +
  xlab("Leadership") +
  ylab("Vulnerability") +
  ggtitle("Word Frequencies")

```


